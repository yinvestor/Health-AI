{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Required Libraries\n",
    "Import the necessary libraries, including pandas, NumPy, and scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Explore the Dataset\n",
    "Load the coronary artery disease dataset and perform exploratory data analysis (EDA) to understand the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 297 entries, 0 to 296\n",
      "Data columns (total 14 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   age        297 non-null    int64  \n",
      " 1   sex        297 non-null    int64  \n",
      " 2   cp         297 non-null    int64  \n",
      " 3   trestbps   297 non-null    int64  \n",
      " 4   chol       297 non-null    int64  \n",
      " 5   fbs        297 non-null    int64  \n",
      " 6   restecg    297 non-null    int64  \n",
      " 7   thalach    297 non-null    int64  \n",
      " 8   exang      297 non-null    int64  \n",
      " 9   oldpeak    297 non-null    float64\n",
      " 10  slope      297 non-null    int64  \n",
      " 11  ca         297 non-null    int64  \n",
      " 12  thal       297 non-null    int64  \n",
      " 13  condition  297 non-null    int64  \n",
      "dtypes: float64(1), int64(13)\n",
      "memory usage: 32.6 KB\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not interpret value `target` for `x`. An entry with this name does not appear in `data`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m     23\u001b[39m plt.figure(figsize=(\u001b[32m8\u001b[39m, \u001b[32m6\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[43msns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcountplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtarget\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m plt.title(\u001b[33m'\u001b[39m\u001b[33mDistribution of Target Variable\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     26\u001b[39m plt.xlabel(\u001b[33m'\u001b[39m\u001b[33mTarget\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\seaborn\\categorical.py:2631\u001b[39m, in \u001b[36mcountplot\u001b[39m\u001b[34m(data, x, y, hue, order, hue_order, orient, color, palette, saturation, fill, hue_norm, stat, width, dodge, gap, log_scale, native_scale, formatter, legend, ax, **kwargs)\u001b[39m\n\u001b[32m   2628\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2629\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCannot pass values for both `x` and `y`.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2631\u001b[39m p = \u001b[43m_CategoricalAggPlotter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2632\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2633\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhue\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2634\u001b[39m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2635\u001b[39m \u001b[43m    \u001b[49m\u001b[43morient\u001b[49m\u001b[43m=\u001b[49m\u001b[43morient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2636\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2637\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlegend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlegend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2638\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2640\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ax \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2641\u001b[39m     ax = plt.gca()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\seaborn\\categorical.py:67\u001b[39m, in \u001b[36m_CategoricalPlotter.__init__\u001b[39m\u001b[34m(self, data, variables, order, orient, require_numeric, color, legend)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m     57\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     58\u001b[39m     data=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     64\u001b[39m     legend=\u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     65\u001b[39m ):\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m     \u001b[38;5;66;03m# This method takes care of some bookkeeping that is necessary because the\u001b[39;00m\n\u001b[32m     70\u001b[39m     \u001b[38;5;66;03m# original categorical plots (prior to the 2021 refactor) had some rules that\u001b[39;00m\n\u001b[32m     71\u001b[39m     \u001b[38;5;66;03m# don't fit exactly into VectorPlotter logic. It may be wise to have a second\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     76\u001b[39m     \u001b[38;5;66;03m# default VectorPlotter rules. If we do decide to make orient part of the\u001b[39;00m\n\u001b[32m     77\u001b[39m     \u001b[38;5;66;03m# _base variable assignment, we'll want to figure out how to express that.\u001b[39;00m\n\u001b[32m     78\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.input_format == \u001b[33m\"\u001b[39m\u001b[33mwide\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m orient \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mh\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\seaborn\\_base.py:634\u001b[39m, in \u001b[36mVectorPlotter.__init__\u001b[39m\u001b[34m(self, data, variables)\u001b[39m\n\u001b[32m    629\u001b[39m \u001b[38;5;66;03m# var_ordered is relevant only for categorical axis variables, and may\u001b[39;00m\n\u001b[32m    630\u001b[39m \u001b[38;5;66;03m# be better handled by an internal axis information object that tracks\u001b[39;00m\n\u001b[32m    631\u001b[39m \u001b[38;5;66;03m# such information and is set up by the scale_* methods. The analogous\u001b[39;00m\n\u001b[32m    632\u001b[39m \u001b[38;5;66;03m# information for numeric axes would be information about log scales.\u001b[39;00m\n\u001b[32m    633\u001b[39m \u001b[38;5;28mself\u001b[39m._var_ordered = {\u001b[33m\"\u001b[39m\u001b[33mx\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}  \u001b[38;5;66;03m# alt., used DefaultDict\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m634\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43massign_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    636\u001b[39m \u001b[38;5;66;03m# TODO Lots of tests assume that these are called to initialize the\u001b[39;00m\n\u001b[32m    637\u001b[39m \u001b[38;5;66;03m# mappings to default values on class initialization. I'd prefer to\u001b[39;00m\n\u001b[32m    638\u001b[39m \u001b[38;5;66;03m# move away from that and only have a mapping when explicitly called.\u001b[39;00m\n\u001b[32m    639\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m var \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mhue\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33msize\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstyle\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\seaborn\\_base.py:679\u001b[39m, in \u001b[36mVectorPlotter.assign_variables\u001b[39m\u001b[34m(self, data, variables)\u001b[39m\n\u001b[32m    674\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    675\u001b[39m     \u001b[38;5;66;03m# When dealing with long-form input, use the newer PlotData\u001b[39;00m\n\u001b[32m    676\u001b[39m     \u001b[38;5;66;03m# object (internal but introduced for the objects interface)\u001b[39;00m\n\u001b[32m    677\u001b[39m     \u001b[38;5;66;03m# to centralize / standardize data consumption logic.\u001b[39;00m\n\u001b[32m    678\u001b[39m     \u001b[38;5;28mself\u001b[39m.input_format = \u001b[33m\"\u001b[39m\u001b[33mlong\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m679\u001b[39m     plot_data = \u001b[43mPlotData\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    680\u001b[39m     frame = plot_data.frame\n\u001b[32m    681\u001b[39m     names = plot_data.names\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\seaborn\\_core\\data.py:58\u001b[39m, in \u001b[36mPlotData.__init__\u001b[39m\u001b[34m(self, data, variables)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m     52\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     53\u001b[39m     data: DataSource,\n\u001b[32m     54\u001b[39m     variables: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, VariableSpec],\n\u001b[32m     55\u001b[39m ):\n\u001b[32m     57\u001b[39m     data = handle_data_source(data)\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     frame, names, ids = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_assign_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m     \u001b[38;5;28mself\u001b[39m.frame = frame\n\u001b[32m     61\u001b[39m     \u001b[38;5;28mself\u001b[39m.names = names\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\seaborn\\_core\\data.py:232\u001b[39m, in \u001b[36mPlotData._assign_variables\u001b[39m\u001b[34m(self, data, variables)\u001b[39m\n\u001b[32m    230\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    231\u001b[39m         err += \u001b[33m\"\u001b[39m\u001b[33mAn entry with this name does not appear in `data`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    235\u001b[39m \n\u001b[32m    236\u001b[39m     \u001b[38;5;66;03m# Otherwise, assume the value somehow represents data\u001b[39;00m\n\u001b[32m    237\u001b[39m \n\u001b[32m    238\u001b[39m     \u001b[38;5;66;03m# Ignore empty data structures\u001b[39;00m\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(val, Sized) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(val) == \u001b[32m0\u001b[39m:\n",
      "\u001b[31mValueError\u001b[39m: Could not interpret value `target` for `x`. An entry with this name does not appear in `data`."
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and Explore the Dataset\n",
    "\n",
    "# Load the coronary artery disease dataset\n",
    "url = './heart_cleveland_upload.csv'  # Replace with the actual URL or file path\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "data.head()\n",
    "\n",
    "# Display basic information about the dataset\n",
    "data.info()\n",
    "\n",
    "# Check for missing values\n",
    "data.isnull().sum()\n",
    "\n",
    "# Display summary statistics of the dataset\n",
    "data.describe()\n",
    "\n",
    "# Visualize the distribution of the target variable\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='target', data=data)\n",
    "plt.title('Distribution of Target Variable')\n",
    "plt.xlabel('Target')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Visualize the correlation matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "correlation_matrix = data.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the Data\n",
    "Handle missing values, encode categorical variables, and scale the features if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the Data\n",
    "\n",
    "# Handle missing values by filling them with the median value of each column\n",
    "data.fillna(data.median(), inplace=True)\n",
    "\n",
    "# Encode categorical variables using one-hot encoding\n",
    "data = pd.get_dummies(data, drop_first=True)\n",
    "\n",
    "# Scale the features using StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(data.drop('target', axis=1))\n",
    "\n",
    "# Create a DataFrame with the scaled features\n",
    "scaled_data = pd.DataFrame(scaled_features, columns=data.columns[:-1])\n",
    "\n",
    "# Add the target variable back to the DataFrame\n",
    "scaled_data['target'] = data['target'].values\n",
    "\n",
    "# Display the first few rows of the preprocessed dataset\n",
    "scaled_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the Data into Training and Testing Sets\n",
    "Split the dataset into training and testing sets using train_test_split from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the Data into Training and Testing Sets\n",
    "\n",
    "# Define the features (X) and the target (y)\n",
    "X = scaled_data.drop('target', axis=1)\n",
    "y = scaled_data['target']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the shapes of the training and testing sets\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Logistic Regression Model\n",
    "Train a logistic regression model using the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Logistic Regression Model\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "logistic_model = LogisticRegression()\n",
    "\n",
    "# Train the model using the training data\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the target values for the test set\n",
    "y_pred = logistic_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n",
    "print('Classification Report:')\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the Model\n",
    "Evaluate the model's performance using metrics such as accuracy, precision, recall, and the ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Model\n",
    "\n",
    "# Import additional necessary libraries for evaluation\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score, roc_curve\n",
    "\n",
    "# Calculate precision and recall\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "\n",
    "# Calculate ROC AUC score\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "# Print precision, recall, and ROC AUC score\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'ROC AUC Score: {roc_auc}')\n",
    "\n",
    "# Plot the ROC curve\n",
    "y_pred_prob = logistic_model.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f'ROC Curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Predictions\n",
    "Use the trained model to make predictions on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scaler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      7\u001b[39m new_data = np.array([[\u001b[32m65\u001b[39m,\u001b[32m1\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m138\u001b[39m,\u001b[32m282\u001b[39m,\u001b[32m1\u001b[39m,\u001b[32m2\u001b[39m,\u001b[32m174\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m1.4\u001b[39m,\u001b[32m1\u001b[39m,\u001b[32m1\u001b[39m,\u001b[32m0\u001b[39m]])\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Scale the new data using the same scaler used for training data\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m new_data_scaled = \u001b[43mscaler\u001b[49m.transform(new_data)\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Make predictions using the trained logistic regression model\u001b[39;00m\n\u001b[32m     14\u001b[39m new_predictions = logistic_model.predict(new_data_scaled)\n",
      "\u001b[31mNameError\u001b[39m: name 'scaler' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Make Predictions\n",
    "\n",
    "# Define new data for prediction (example data)\n",
    "# new_data = np.array([[63, 1, 3, 145, 233, 1, 0, 150, 0, 2.3, 0, 0, 1]])\n",
    "new_data = np.array([[65,1,0,138,282,1,2,174,0,1.4,1,1,0]])\n",
    "\n",
    "\n",
    "# Scale the new data using the same scaler used for training data\n",
    "new_data_scaled = scaler.transform(new_data)\n",
    "\n",
    "# Make predictions using the trained logistic regression model\n",
    "new_predictions = logistic_model.predict(new_data_scaled)\n",
    "\n",
    "# Print the predictions\n",
    "print(f'Predictions for the new data: {new_predictions}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
