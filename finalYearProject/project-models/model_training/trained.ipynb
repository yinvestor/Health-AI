{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing\n",
    "First, let's load and preprocess the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    age  sex   cp  trestbps   chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
      "0  63.0  1.0  1.0     145.0  233.0  1.0      2.0    150.0    0.0      2.3   \n",
      "1  67.0  1.0  4.0     160.0  286.0  0.0      2.0    108.0    1.0      1.5   \n",
      "2  67.0  1.0  4.0     120.0  229.0  0.0      2.0    129.0    1.0      2.6   \n",
      "3  37.0  1.0  3.0     130.0  250.0  0.0      0.0    187.0    0.0      3.5   \n",
      "4  41.0  0.0  2.0     130.0  204.0  0.0      2.0    172.0    0.0      1.4   \n",
      "\n",
      "   slope   ca thal  target  \n",
      "0    3.0  0.0  6.0       0  \n",
      "1    2.0  3.0  3.0       2  \n",
      "2    2.0  2.0  7.0       1  \n",
      "3    3.0  0.0  3.0       0  \n",
      "4    1.0  0.0  3.0       0  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 303 entries, 0 to 302\n",
      "Data columns (total 14 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   age       303 non-null    float64\n",
      " 1   sex       303 non-null    float64\n",
      " 2   cp        303 non-null    float64\n",
      " 3   trestbps  303 non-null    float64\n",
      " 4   chol      303 non-null    float64\n",
      " 5   fbs       303 non-null    float64\n",
      " 6   restecg   303 non-null    float64\n",
      " 7   thalach   303 non-null    float64\n",
      " 8   exang     303 non-null    float64\n",
      " 9   oldpeak   303 non-null    float64\n",
      " 10  slope     303 non-null    float64\n",
      " 11  ca        303 non-null    object \n",
      " 12  thal      303 non-null    object \n",
      " 13  target    303 non-null    int64  \n",
      "dtypes: float64(11), int64(1), object(2)\n",
      "memory usage: 33.3+ KB\n",
      "None\n",
      "              age         sex          cp    trestbps        chol         fbs  \\\n",
      "count  303.000000  303.000000  303.000000  303.000000  303.000000  303.000000   \n",
      "mean    54.438944    0.679868    3.158416  131.689769  246.693069    0.148515   \n",
      "std      9.038662    0.467299    0.960126   17.599748   51.776918    0.356198   \n",
      "min     29.000000    0.000000    1.000000   94.000000  126.000000    0.000000   \n",
      "25%     48.000000    0.000000    3.000000  120.000000  211.000000    0.000000   \n",
      "50%     56.000000    1.000000    3.000000  130.000000  241.000000    0.000000   \n",
      "75%     61.000000    1.000000    4.000000  140.000000  275.000000    0.000000   \n",
      "max     77.000000    1.000000    4.000000  200.000000  564.000000    1.000000   \n",
      "\n",
      "          restecg     thalach       exang     oldpeak       slope      target  \n",
      "count  303.000000  303.000000  303.000000  303.000000  303.000000  303.000000  \n",
      "mean     0.990099  149.607261    0.326733    1.039604    1.600660    0.937294  \n",
      "std      0.994971   22.875003    0.469794    1.161075    0.616226    1.228536  \n",
      "min      0.000000   71.000000    0.000000    0.000000    1.000000    0.000000  \n",
      "25%      0.000000  133.500000    0.000000    0.000000    1.000000    0.000000  \n",
      "50%      1.000000  153.000000    0.000000    0.800000    2.000000    0.000000  \n",
      "75%      2.000000  166.000000    1.000000    1.600000    2.000000    2.000000  \n",
      "max      2.000000  202.000000    1.000000    6.200000    3.000000    4.000000  \n",
      "age         0\n",
      "sex         0\n",
      "cp          0\n",
      "trestbps    0\n",
      "chol        0\n",
      "fbs         0\n",
      "restecg     0\n",
      "thalach     0\n",
      "exang       0\n",
      "oldpeak     0\n",
      "slope       0\n",
      "ca          0\n",
      "thal        0\n",
      "target      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the dataset\n",
    "column_names = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', \n",
    "                'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target']\n",
    "df = pd.read_csv('processed.cleveland.csv', names=column_names)\n",
    "print(df.head())\n",
    "print(df.info())\n",
    "print(df.describe())\n",
    "df.dtypes\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Convert target to binary (0 = no disease, 1 = disease)\n",
    "df['target'] = df['target'].apply(lambda x: 0 if x == 0 else 1)\n",
    "\n",
    "# Handle missing values (marked as '?')\n",
    "df = df.replace('?', np.nan)\n",
    "df = df.dropna()\n",
    "\n",
    "# Split into features and target\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training and Fine-Tuning\n",
    "Now let's train and optimize each model:\n",
    "\n",
    "1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.87\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89        36\n",
      "           1       0.83      0.83      0.83        24\n",
      "\n",
      "    accuracy                           0.87        60\n",
      "   macro avg       0.86      0.86      0.86        60\n",
      "weighted avg       0.87      0.87      0.87        60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Logistic Regression with hyperparameter tuning\n",
    "lr_params = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear']\n",
    "}\n",
    "\n",
    "lr_grid = GridSearchCV(LogisticRegression(max_iter=1000), lr_params, cv=5, scoring='accuracy')\n",
    "lr_grid.fit(X_train, y_train)\n",
    "\n",
    "# Best model\n",
    "best_lr = lr_grid.best_estimator_\n",
    "lr_pred = best_lr.predict(X_test)\n",
    "lr_accuracy = accuracy_score(y_test, lr_pred)\n",
    "\n",
    "print(f\"Logistic Regression Accuracy: {lr_accuracy:.2f}\")\n",
    "print(classification_report(y_test, lr_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.90\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92        36\n",
      "           1       0.88      0.88      0.88        24\n",
      "\n",
      "    accuracy                           0.90        60\n",
      "   macro avg       0.90      0.90      0.90        60\n",
      "weighted avg       0.90      0.90      0.90        60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# SVM with hyperparameter tuning\n",
    "svm_params = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [1, 0.1, 0.01, 0.001],\n",
    "    'kernel': ['rbf', 'linear']\n",
    "}\n",
    "\n",
    "svm_grid = GridSearchCV(SVC(), svm_params, cv=5, scoring='accuracy')\n",
    "svm_grid.fit(X_train, y_train)\n",
    "\n",
    "# Best model\n",
    "best_svm = svm_grid.best_estimator_\n",
    "svm_pred = best_svm.predict(X_test)\n",
    "svm_accuracy = accuracy_score(y_test, svm_pred)\n",
    "\n",
    "print(f\"SVM Accuracy: {svm_accuracy:.2f}\")\n",
    "print(classification_report(y_test, svm_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. XGBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Accuracy: 0.85\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.86      0.87        36\n",
      "           1       0.80      0.83      0.82        24\n",
      "\n",
      "    accuracy                           0.85        60\n",
      "   macro avg       0.84      0.85      0.84        60\n",
      "weighted avg       0.85      0.85      0.85        60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %pip install --upgrade xgboost scikit-learn\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# XGBoost with hyperparameter tuning\n",
    "xgb_params = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "xgb_grid = GridSearchCV(XGBClassifier(random_state=42), xgb_params, cv=5, scoring='accuracy')\n",
    "xgb_grid.fit(X_train, y_train)\n",
    "\n",
    "# Best model\n",
    "best_xgb = xgb_grid.best_estimator_\n",
    "xgb_pred = best_xgb.predict(X_test)\n",
    "xgb_accuracy = accuracy_score(y_test, xgb_pred)\n",
    "\n",
    "print(f\"XGBoost Accuracy: {xgb_accuracy:.2f}\")\n",
    "print(classification_report(y_test, xgb_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.85\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.86      0.87        36\n",
      "           1       0.80      0.83      0.82        24\n",
      "\n",
      "    accuracy                           0.85        60\n",
      "   macro avg       0.84      0.85      0.84        60\n",
      "weighted avg       0.85      0.85      0.85        60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Random Forest with hyperparameter tuning\n",
    "rf_params = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 5, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "rf_grid = GridSearchCV(RandomForestClassifier(random_state=42), rf_params, cv=5, scoring='accuracy')\n",
    "rf_grid.fit(X_train, y_train)\n",
    "\n",
    "# Best model\n",
    "best_rf = rf_grid.best_estimator_\n",
    "rf_pred = best_rf.predict(X_test)\n",
    "rf_accuracy = accuracy_score(y_test, rf_pred)\n",
    "\n",
    "print(f\"Random Forest Accuracy: {rf_accuracy:.2f}\")\n",
    "print(classification_report(y_test, rf_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 0.77\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.78      0.80        36\n",
      "           1       0.69      0.75      0.72        24\n",
      "\n",
      "    accuracy                           0.77        60\n",
      "   macro avg       0.76      0.76      0.76        60\n",
      "weighted avg       0.77      0.77      0.77        60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Decision Tree with hyperparameter tuning\n",
    "dt_params = {\n",
    "    'max_depth': [None, 5, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "dt_grid = GridSearchCV(DecisionTreeClassifier(random_state=42), dt_params, cv=5, scoring='accuracy')\n",
    "dt_grid.fit(X_train, y_train)\n",
    "\n",
    "# Best model\n",
    "best_dt = dt_grid.best_estimator_\n",
    "dt_pred = best_dt.predict(X_test)\n",
    "dt_accuracy = accuracy_score(y_test, dt_pred)\n",
    "\n",
    "print(f\"Decision Tree Accuracy: {dt_accuracy:.2f}\")\n",
    "print(classification_report(y_test, dt_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Model Deployment Recommendation\n",
    "For production deployment, I recommend using the XGBoost model as it provides the best balance of accuracy and performance. Here's how to save and load the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heart Disease Prediction (0 = No, 1 = Yes): 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bigir\\anaconda3\\envs\\spyder-env\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model and scaler\n",
    "joblib.dump(best_xgb, 'heart_disease_xgboost_model.pkl')\n",
    "joblib.dump(scaler, 'heart_disease_scaler.pkl')\n",
    "\n",
    "# Later, to load and use the model\n",
    "loaded_model = joblib.load('heart_disease_xgboost_model.pkl')\n",
    "loaded_scaler = joblib.load('heart_disease_scaler.pkl')\n",
    "\n",
    "# Example prediction\n",
    "# new_data = np.array([[63, 1, 1, 145, 233, 1, 2, 150, 0, 2.3, 3, 0, 6]])  # example patient data\n",
    "new_data = np.array([[65,1,0,138,282,1,2,174,0,1.4,1,1,0]])\n",
    "scaled_data = loaded_scaler.transform(new_data)\n",
    "prediction = loaded_model.predict(scaled_data)\n",
    "print(\"Heart Disease Prediction (0 = No, 1 = Yes):\", prediction[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spyder-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
